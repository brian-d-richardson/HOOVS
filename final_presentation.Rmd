---
title: "BIOS 735 Final Presentation"
output: html_document
author: "Ben Bodek, Brian Chen, Forrest Hurley, Brian Richardson, Emmanuel Rockwell"
date: "04/28/22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

suppressPackageStartupMessages(if (!require(dplyr)){install.packages("dplyr")})
suppressPackageStartupMessages(if (!require(tidyr)){install.packages("tidyr")})
suppressPackageStartupMessages(if (!require(ggplot2)){install.packages("ggplot2")})
suppressPackageStartupMessages(if (!require(ordinalNet)){install.packages("ordinalNet")})
suppressPackageStartupMessages(if (!require(foreign)){install.packages("foreign")})
suppressPackageStartupMessages(if (!require(devtools)){install.packages("devtools")})
suppressPackageStartupMessages(if (!require(tictoc)){install.packages("tictoc")})
suppressPackageStartupMessages(if (!require(psych)){install.packages("psych")})
suppressPackageStartupMessages(if (!require(caret)){install.packages("caret")})
suppressPackageStartupMessages(if (!require(Boruta)){install.packages("Boruta")})
suppressPackageStartupMessages(if (!require(ordinalForest)){install.packages("ordinalForest")})
suppressPackageStartupMessages(if (!require(cvms)){install.packages("cvms")})
load_all()

#For reproducibility
set.seed(1)
```

# Introduction

## Data Description

The dataset we will use for our project is from the Americans' Changing Lives (ACL) longitudinal survey that tracks sociological, psychological, mental, and physical health items. The study has a primary focus focus in differences between Black and White Americans. The first wave of surveys was conducted in 1986 and included 3,617 adults ages 25 and up, with Black Americans and people aged 60 and over over-sampled at twice the rate of the others. The survivors at each measurement were re-interviewed in 1989, 1994, 2002, and 2011, called Waves II, III, IV, and V, respectively. 

## Our Aims for the project

The overarching goal of this project will be evaluating the association of the available demographic, sociographic, and psychographic variables with participantsâ€™ self reported life satisfaction. Specifically, we aim to identify factors which are predictive of high or low satisfaction with life. Because life satisfaction is an ordinal variable, with responses ranging from 1. Completely satisfied to 2. Not at all satisfied, our analysis methods must be designed to handle ordinal outcomes to avoid loss of information. Furthermore, the ACL survey is a high dimensional dataset. Therefore, in order to conduct the usual inference with a generalized linear model, we must first reduce this dimensionality.

To achieve the goal of identifying factors associated with life satisfaction, we aim to reduce the dimentionality, with two methods: ordinal LASSO regression and Random Forests. 

# Methods

## Data Cleaning and Preprocessing

Although the dataset contains longitudinal data, our project will focus on a cross-sectional analysis of Wave I data. Moreover, we manually select for variables....

Split data into training and test set.

## Ordinal Regression Model:

Suppose that, for observation $i = 1, \dots, n$, the ordinal outcome $Y_i$ given the covariate vector $\pmb{x}_i$ has a multinomial distribution with $J$ outcome categories and probabilities of success $\pi_1(\pmb{x}_i), \dots, \pi_J(\pmb{x}_i)$. That is,

$$Y_i | \pmb{x}_i \sim \text{multinomial} \{ 1; \pi_1(\pmb{x}_i), \dots, \pi_J(\pmb{x}_i) \}.$$

The cumulative probability for subject $i$ and ordinal outcome category $j$ is $P(Y_i \leq j | \pmb{x}_i) = \sum_{k=1}^j \pi_k(\pmb{x}_i)$. Note that by definition $P(Y_i \leq J | \pmb{x}_i) = 1$.

The following proportional odds model relates the cumulative probability for subject $i$ and ordinal outcome category $j$ to the covariates $\pmb{x}_i$ via the parameters $\pmb{\alpha} = (\alpha_1, \dots, \alpha_{J-1})^T$ and $\pmb{\beta} = (\beta_1, \dots, \beta_p)^T$ with a logit link function.

$$ \text{logit}\{ P(Y_i \leq j | \pmb{x}_i) \} = \alpha_j + \pmb{x}_i^T \pmb{\beta}. $$

In this model, $\alpha_1, \dots, \alpha_{J-1}$ are outcome category-specific intercepts for the first $J-1$ ordinal outcome categories and $\beta_1, \dots, \beta_p$ are the slopes corresponding to the $p$ covariates. Since the cumulative probabilities must be nondecreasing in $j$, i.e., is $P(Y_i \leq j | \pmb{x}_i) < P(Y_i \leq j+1 | \pmb{x}_i)$, we require that $\alpha_1 < \dots < \alpha_{J-1}$.

### Estimation

The likelihood function for the ordinal regression model is

$$ L_n(\pmb{\alpha}, \pmb{\beta}) = \prod_{i=1}^n \prod_{j=1}^J \left\{ \text{logit}^{-1}(\alpha_j + \pmb{x}_i^T\pmb{\beta}) - \text{logit}^{-1}(\alpha_{j-1} + \pmb{x}_i^T\pmb{\beta} )   \right\} ^ {_(y_i = j)}. $$

Let $l(\pmb{\alpha}, \pmb{\beta}) = \frac{-1}{n} \log L(\pmb{\alpha}, \pmb{\beta})$ be the standardized log-likelihood.

The LASSO-penalized ordinal regression model is fit by minimizing the following objective function with respect to $\pmb{\alpha}$ and $\pmb{\beta}$.

$$ f(\pmb{\alpha}, \pmb{\beta}) = l(\pmb{\alpha}, \pmb{\beta}) + \lambda\sum_{j=1}^p|\beta_j|. $$

### Proximal Gradient Descent Algorithm

The objective function can be minimized using a proximal gradient descent (PGD) algorithm.

Fix the following initial parameters for the PGD algorithm: $m > 0$ (the initial step size), $a \in (0, 1)$ (the step size decrement value), and $\epsilon > 0$ (the convergence criterion).

The proximal projection operator for the LASSO penalty (applied to $\pmb{\beta}$ but not to $\pmb{\alpha}$) is

$$ \text{prox}_{\lambda m}(\pmb{w}, \pmb{z}) = \text{argmin}_{\pmb{\alpha}, \pmb{\beta}} \frac{1}{2m} \left(||\pmb{w} - \pmb{\alpha} ||_2^2 + ||\pmb{z} - \pmb{\beta} ||_2^2 \right) + \lambda\sum_{j=1}^p|\beta_j| = \left\{ \pmb{w},  \text{sign}(\pmb{z})(\pmb{z} - m \lambda)_+ \right\} $$

Given current estimates $\pmb{\theta}^{(k)} = (\pmb{\alpha}^{(k)}, \pmb{\beta}^{(k)})^T$, search for updated estimates $\pmb{\theta}^{(k+1)}$ by following the steps:

1)  propose a candidate update $\pmb{\theta} = \text{prox}_{\lambda m}\left\{\pmb{\theta}^{(k)} - \frac{1}{m} \nabla l(\pmb{\theta}^{(k)})\right\}$,

2)  if the condition $l(\pmb{\theta}) \leq l(\pmb{\theta}^{(k)}) + \nabla l(\pmb{\theta}^{(k)})^T(\pmb{\theta} - \pmb{\theta}^{(k)}) + \frac{1}{2m} (\pmb{\theta} - \pmb{\theta}^{(k)})^T (\pmb{\theta} - \pmb{\theta}^{(k)})$ is met, then make the the update $\pmb{\theta}^{(k+1)} = \pmb{\theta}$,

3)  else decrement the step size $m = am$ and returning to step 2.

Continue updating $\pmb{\theta}^{(k)}$ until convergence, i.e., until $\left|\frac{f(\pmb{\theta}^{(k+1)}) - f(\pmb{\theta}^{(k)})}{f(\pmb{\theta}^{(k)})}\right| < \epsilon$.

A technical note is that the $\pmb{\alpha}$ parameters are constrained by $\alpha_1 < \dots < \alpha_{J-1}$. We can reparametrize the model with $\pmb{\zeta} = (\zeta_1, \dots, \zeta_{J-1})^T$, where $\zeta_1 = \alpha_1$ and $\zeta_j = \log(\alpha_j - \alpha_{j-1})$ for $j = 2, \dots, J-1$. Then $\pmb{\zeta} \in \mathbb{R}^{J-1}$ have no constraints. So we can follow the above procedure to minimize the above objective function with respect to $\pmb{\zeta}$ and $\pmb{\beta}$, then back-transform to obtain estimates for $\pmb{\alpha}$.

## Random Forest


# Results

## Simulation Study

```{r, echo=FALSE}

# sample size
n <- 1000

# number of covariates
p <- 50

# number of categories for ordinal outcome
J <- 4

# grid of lambdas
lambdas <- seq(0, 0.2, 0.02)

# set population parameters
alpha <- seq(.5, 4, length = J - 1) # category-specific intercepts
beta <- rep(0, p)                     # slope parameters
beta[1: floor(p / 2)] <- 1            # half of the betas are 0, other half are 1

# simulate data according to the above parameters
dat <- simulate.data(
  n = 1000,
  alpha = alpha,
  beta = beta)

```

For this example, we simulated data with $n$ = `r n` observations, $p$ = `r p` covariates, $J$ = `r J` ordinal outcome categories, and true parameter values of $\pmb{\alpha}_0$ = (`r alpha`) and $\pmb{\beta}_0$ = (`r beta`). Note that this implies the first half of the covariates are truly associated with the outcome and the last half are not. The first 10 rows and 10 columns of the data set are shown below.

```{r, echo=FALSE}

dat[1:10, 1:10] %>% 
  mutate_if(.predicate = function(x) is.numeric(x),
            .funs = function(x) round(x, digits = 2))
```

### Run ordinal Regression

Now run our version of a LASSO-penalized ordinal regression function on the simulated data for various values of $\lambda$: `r lambdas`.

```{r}

# test our LASSO-penalized ordinal regression function
tic("our ordreg.lasso() function")
res.ordreg <- ordreg.lasso(
  formula = y ~ .,
  data = dat,
  lambdas 
)
toc()

```

### Comparison to existing method

We can compare our model with an existing function `ordinalNet::ordinalNet()` and fit the same models with the same $\lambda$ values.

```{r}

# compare with ordinalNet results
tic("ordinalNet() function")
res.ordnet <- ordinalNet::ordinalNet(
  x = as.matrix(dat[, -1]),
  y = dat[, 1],
  alpha = 1, # alpha = 1 corresponds to LASSO
  lambdaVals = lambdas,
  link = "logit",
  family = "cumulative"
)
toc()

```

Plotting the estimates from each method against, we see that all the parameters do not deviate far from the $45^{\circ}$ line.

```{r, echo=FALSE}

coef.ordreg <- cbind(res.ordreg$alpha, res.ordreg$beta)

# need to reverse the order of rows to match output
coef.ordnet <- as.matrix(res.ordnet$coefs)[rev(1:length(lambdas)),]

# create data frame with coefficient estimates from both methods
coef.wide <- rbind(cbind("ordreg", lambdas, coef.ordreg),
                   cbind("ordnet", lambdas, coef.ordnet)) %>% 
  as.data.frame() %>% 
  `colnames<-`(c("method",
                 "lambda",
                 paste0("alpha", 1:(J-1)),
                 paste0("beta", 1:p))) %>% 
  mutate_at(.vars = vars(-("method")),
            .funs = as.numeric)

coef.long <- coef.wide %>% 
  pivot_longer(cols = -c(method, lambda)) %>% 
  mutate(type = factor(ifelse(grepl("alpha", name, fixed = TRUE),
                              "alpha", "beta")))

ggplot(NULL,
       aes(x = coef.long$value[coef.long$method == "ordnet"],
           y = coef.long$value[coef.long$method == "ordreg"])) +
  geom_point(size = 3,
             shape = 1) +
  geom_abline(color = "blue",
              linetype = "dashed") +
  labs(x = "Estimates from ordinalNet Function",
       y = "Estimates from ordreg_lasso Function") +
  ggtitle("Comparison of LASSO-Penalized Ordinal Regression Methods") +
  theme_bw()


```


We can now look at how the parameter estimates from our funciton change as the penalty parameter $\lambda$ changes

```{r , echo=FALSE}

ggplot(data = filter(coef.long, method == "ordreg"),
       aes(x = lambda,
           y = value,
           group = name,
           color = type)) +
  geom_line() +
  labs(x = "Lambda",
       y = "LASSO-Penalized Estimate",
       color = "Parameter\nType") +
  ggtitle("LASSO-Estimates vs Penalization Parameter Lambda") +
  theme_bw()

```

Note in the above plot that the $\pmb{\alpha}$ estimates do not shrink all the way to 0 since they are not penalized in the LASSO model. On the other hand, the $\pmb{\beta}$ estimates do shrink to 0 as $\lambda$ increases. Recall that the data were simulated according to a model where half of the $\pmb{\beta}$ values are truly 0 and the other half are truly 1. It is clear in the above plot which covariates are truly not associated with the outcome based on how fast their corresponding parameter estimates shrink to 0.


## Real Data Analysis 

```{r}
# Note that data must be downloaded and processed (instructions in data_processing.R)
# prior to running this step

# reading in processed data
data <- fread('processed_data.csv')

y_factor_key <- data.frame(scaled = sort(unique(data$`B1:SATISFACTION W/LIFE`)),
                           y = factor(1:5, ordered = T))

data_ordered_y <- merge(y_factor_key, data, by.x = "scaled", by.y = "B1:SATISFACTION W/LIFE") %>% 
  arrange(`W1 Case Id`) %>%
  select(-c("scaled", "W1 Case Id"))
```

We start by defining our outcome and covariates. Our outcome of interest is the participants' **Satistfaction with Life**, an ordinal survey response ranging from 1 = "Completely Satisfied" to 5 = "Not at all Satisfied".

The covariates are defined as all other survey questions. 

```{r}
# set outcome as Life Satisfaction, "B1.SATISFACTION.W.LIFE"
y<-data$`B1:SATISFACTION W/LIFE`
# create covariate matrix
x<-select(data, select = -c("B1:SATISFACTION W/LIFE"))
```


Below is a table of percentage of responses by ordinal category. We can see that most responses
fall into category 2, "Very satisfied with life".

```{r}
l<-table(y)/sum(!is.na(y))*100
barplot(l)
```
```{r}
set.seed(5)

#Create training and testing set
train_test_i <- createDataPartition(y = data_ordered_y$y, p = 0.6, list = F)

data_train <- data_ordered_y[train_test_i,]
data_test <- data_ordered_y[-train_test_i,]

#Within training set, create folds
k <- 5 #set number of folds 
fold_indicies <- createFolds(y = data_train$y, k=k)
```

### Ordinal Regression LASSO

### Random Forest

```{r}

#Fit Ordinal Random forest
tic(paste0("Fold #", fold))
rf<-randomforest.features("y",data_train)
toc()
  
# extract important features
imp.features<-rf$important.features

# train model on selected features using training dataset
model<-ordinalForest::ordfor("y", data_train[,c("y",imp.features)])

## save predictions on cross validation test set
predicted <- predict(model,data_test[,-1])
    
#Calculate Kappa
test_kappa <- cohen.kappa(x = matrix(c(data_test[,1], predicted[-length(predicted)]),
                                           ncol = 2, 
                                           byrow = F))$weighted.kappa
    

```


```{r}
library(cvms)
y = data_test[,1]
y.pred = predicted[-length(predicted)]$ypred
y.pred <- factor(y.pred, order=T)


```


```{r}
imp.features

```



# Conclusion

