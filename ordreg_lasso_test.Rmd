---
title: "Test of Ordinal Regression with LASSO"
author: "Brian Richardson"
output: html_document
---

```{r setup, include=FALSE}

rm(list = ls())

library(dplyr)
library(MASS)
library(foreign)
library(ggplot2)
library(devtools)
library(kableExtra)
load_all()

set.seed(1)

```

### Ordinal Regression Model:

Suppose that, for observation $i = 1, \dots, n$, the ordinal outcome $Y_i$ given the covariate vector $\pmb{x}_i$ has a multinomial distribution with $J$ outcome categories and probabilities of success $\pi_1(\pmb{x}_i), \dots, \pi_J(\pmb{x}_i)$. That is,

$$Y_i | \pmb{x}_i \sim \text{multinomial} \{ 1; \pi_1(\pmb{x}_i), \dots, \pi_J(\pmb{x}_i) \}.$$

The cumulative probability for subject $i$ and ordinal outcome category $j$ is $P(Y_i \leq j | \pmb{x}_i) = \sum_{k=1}^j \pi_k(\pmb{x}_i)$. Note that by definition $P(Y_i \leq J | \pmb{x}_i) = 1$.

The following proportional odds model relates the cumulative probability for subject $i$ and ordinal outcome category $j$ to the covariates $\pmb{x}_i$ via the parameters $\pmb{\alpha} = (\alpha_1, \dots, \alpha_{J-1})^T$ and $\pmb{\beta} = (\beta_1, \dots, \beta_p)^T$ with a logit link function.

$$ \text{logit}\{ P(Y_i \leq j | \pmb{x}_i) \} = \alpha_j + \pmb{x}_i^T \pmb{\beta}. $$

In this model, $\alpha_1, \dots, \alpha_{J-1}$ are outcome category-specific intercepts for the first $J-1$ ordinal outcome categories and $\beta_1, \dots, \beta_p$ are the slopes corresponding to the $p$ covariates. Since the cumulative probabilities must be nondecreasing in $j$, i.e., is $P(Y_i \leq j | \pmb{x}_i) <= P(Y_i \leq j+1 | \pmb{x}_i)$, we require that $\alpha_1 < \dots < \alpha_{J-1}$.

### Data Generation

We begin by simulating a data set using the `sim.data()` function. For $n$ subjects, we generate $p$ covariates from independent standard normal distributions. Given true parameters $\pmb{\alpha}_0$ and $\pmb{\beta}_0$, we compute the multinomial probabilities for the outcome for each individual and simulate $y_i$ accordingly.

```{r}

# sample size
n <- 1000

# number of covariates
p <- 10

# number of categories for ordinal outcome
J <- 4

# set population parameters
alpha0 <- seq(.5, 1.5, length = J - 1) # category-specific intercepts
beta0 <- c(rep(1, 4), # slopes
           rep(0, p - 4)) 

# simulate data according to the above parameters
dat <- sim.data(formula = y ~ .,
                n = 1000,
                alpha = alpha0,
                beta = beta0)

```

For this example, we simulated data with $n$ = `r n` observations, $p$ = `r p` covariates, $J$ = `r J` ordinal outcome categories, and true parameter values of $\pmb{\alpha}_0$ = (`r alpha0`) and $\pmb{\beta}_0$ = (`r beta0`). Note that this implies the first 4 covariates are truly associated with the outcome and the last 6 are not. The first 10 rows of the data set are shown below.

```{r}

dat[1:10, ] %>% 
  mutate_if(.predicate = function(x) is.numeric(x),
            .funs = function(x) round(x, digits = 2))

```

### Estimation

The likelihood function for the ordinal regression model is

$$ l(\pmb{\alpha}, \pmb{\beta}) = \prod_{i=1}^n \prod_{j=1}^J \left\{ \text{logit}^{-1}(\alpha_j + \pmb{x}_i^T\pmb{\beta}) - \text{logit}^{-1}(\alpha_{j-1} + \pmb{x}_i^T\pmb{\beta} )   \right\} ^ {_(y_i = j)}. $$

The log-likelihood can be maximized numerically using the BFGS method in `optim`. To ensure that the MLEs for $\pmb{\alpha}$ satisfy the constraint $\alpha_1 < \dots < \alpha_{J-1}$, we can reparametrize the model with $\pmb{\zeta} = (\zeta_1, \dots, \zeta_{J-1})^T$, where $\zeta_1 = \alpha_1$ and $\zeta_j = \log(\alpha_j - \alpha_{j-1})$ for $j = 2, \dots, J-1$.

The asymptotic covariance matrix of the parameter estimates can be obtained by inverting the negative hessian of the log-likelihood function evaluated at the MLEs.

### LASSO Penalization

To avoid overfitting and encourage parsimony in our model, we can fit this ordinal regression model with a LASSO penalty. This involves maximizing an objective function that is the sum of the log-likelihood and an $L^1$ penalty term.

$$ f(\pmb{\alpha}, \pmb{\beta}) = \frac{1}{n} l(\pmb{\alpha}, \pmb{\beta}) - \lambda\sum_{k = 1}^p |\beta_k| $$ \

### Implementation

We can now test our own version of an ordinal regression function on the simulated data set with and without a LASSO penalty.

```{r}

# test our ordinal regression function with no LASSO penalty
ord.mod.lambda0 <- ordreg.lasso(formula = y ~ .,
                                data = dat,
                                lambda = 0)
ord.mod.lambda0

# test our ordinal regression function with a LASSO penalty
ord.mod.lambda1 <- ordreg.lasso(formula = y ~ .,
                                data = dat,
                                lambda = 0.05)
ord.mod.lambda1

```

We then use an existing function `MASS::polr()` to run an ordinal logistic regression model on the simulated data set.

```{r}

# example of ordinal regression using existing function MASS::polr
ord.mod.polr <- polr(polr(y ~ ., data = dat))
summary(ord.mod.polr)
logLik(ord.mod.polr)

```

We can now compare the parameter estimates between the 3 methods for fitting the model.

```{r}

data.frame("Parameter" = c(paste0("alpha", 1:(J-1)),
                           paste0("beta", 1:p)),
           "Truth" = c(alpha0, beta0),
           "polr" = round(c(ord.mod.polr$zeta, -1 * ord.mod.polr$coefficients), 3),
           "ordreg.lasso.0" = round(c(ord.mod.lambda0$alpha, ord.mod.lambda0$beta), 3),
           "ordreg.lasso.1" = round(c(ord.mod.lambda1$alpha, ord.mod.lambda1$beta), 3)) %>%
  kable(col.names = c("Paremeter", "Truth", "MASS:polr()",
                      "ordreg.lasso (lambda = 0)",
                      "ordreg.lasso (lambda = 0.05)"),
        caption = "Ordinal Regression Model Estimates by Method") %>% 
  kable_classic(full_width = F)
  


```
